# -*- coding: utf-8 -*-
"""BertModelMulticlass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vphteo7e1OvG_BJm4B3dsWUddb_ztP0d
"""

!pip install transformers

import pandas as pd
import numpy as np
from transformers import BertTokenizer

df = pd.read_excel('/content/drive/MyDrive/articles bias prediction/ContentCatFinalDataset.xlsx')   #save the excel dataset as df
df.head() #show the first 5 values of the table

seq_len = 512 #sequence length of the tokenized sequence
num_samples = len(df) #number of samples is equal to the number of news. 
Xids = np.zeros((num_samples, seq_len)) #initialize input id to zero 
Xmask = np.zeros((num_samples, seq_len)) #initialize max to zero
Xids.shape #show the number of samples and the number of tokens within each sample

tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased') #load the tokenizer from the model pretrained on hugging faces bert-base-multilingual-cased
for i, Content in enumerate(df['Content']): #loop through every content within the content column by getting the row number
        tokens = tokenizer.encode_plus(Content, max_length=seq_len, truncation=True, #text longer than 512 tokens will be truncated the model should be consisted and not have different shapes
                                        padding='max_length', add_special_tokens=True,#if the text is shor than 512 the remain will be padding to 512, special tokens are CLS : start sequence, sep: mark the end of a sequence, pad : padding token
                                        return_tensors='tf') # return tensor
        Xids[i, :] = tokens['input_ids'] 
        Xmask[i :] = tokens['attention_mask']

Xids #sequence start, content end with padding

Xmask # bert attention

possible_labels = df.Value.unique() #store the number of the bias. 

label_dict = {}
for index, possible_label in enumerate(possible_labels):
    label_dict[possible_label] = index
label_dict

df['Values'] = df.Value.replace(label_dict) #add a new column called values with the number of the bias

df

arr = df['Values'].values #create an array with the values of column values. 
arr

labels = np.zeros((num_samples, arr.max()+1)) #initialize a zero array with number of samples and max values. 
labels.shape #shows the length of the dataframe and classes

labels[np.arange(num_samples), arr] = 1

labels

import tensorflow as tf

dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels)) #object tensor

dataset.take(1) #show the an example

Xids[0,:].shape

def map_func(input_ids, masks, labels): #merge inputs mask and labels 
    return {'input_ids': input_ids, 'attention_mask': masks}, labels #map the correct tensor

dataset = dataset.map(map_func)

dataset.take(1) #merge every in a tuple and dictionary

batch_size = 3 
dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)  #shuffle batches, 

dataset.take(1)

split =0.9 #90 % will be training data and 10% validation data  

size = int((num_samples / batch_size) * split) # size

train_ds = dataset.take(size) # take the first 11,266 news
val_ds = dataset.skip(size) #skip the first 11,266 news and the remainder is validation

del dataset

from transformers import TFAutoModel

bert = TFAutoModel.from_pretrained('bert-base-multilingual-cased')

bert.summary()

input_ids = tf.keras.layers.Input(shape=(seq_len), name='input_ids', dtype='int32') #shape of the input_ids layers
mask = tf.keras.layers.Input(shape=(seq_len), name='attention_mask', dtype='int32') #shape of the mask layers

embeddings = bert.bert(input_ids, attention_mask=mask)[1] #pass out input ids and attention mask to bert

x = tf.keras.layers.Dense(1024, activation='relu')(embeddings) #convert labels into input prediction 
y = tf.keras.layers.Dense(arr.max()+1, activation='softmax', name='outputs')(x) #final layer has 3 classes

model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)
model.summary()

optimizer = tf.keras.optimizers.Adam(lr=1e-5, decay= 1e-6) 
loss = tf.keras.losses.CategoricalCrossentropy()
acc = tf.keras.metrics.CategoricalAccuracy('accuracy')

model.compile(optimizer=optimizer, loss=loss, metrics=[acc]) #model compile with our trainning parameters

history = model.fit(
    train_ds, #training dataset
    validation_data=val_ds, #validation data
    epochs=5 #training epochs
)

model.save('/content/drive/MyDrive/articles bias prediction/CapstoneBertModel') #save the model

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=1
)

model.save('/content/drive/MyDrive/articles bias prediction/CapstoneBertModel')#save the model